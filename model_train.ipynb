{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d767f8d5",
   "metadata": {},
   "source": [
    "### Making necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7018de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f28a2",
   "metadata": {},
   "source": [
    "### Importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f12dfe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d419848c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81eba679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.DataFrame(dataset['train'])\n",
    "test_data = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77c01302",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data['text']\n",
    "y_train = train_data['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(test_data['text'], test_data['label'], test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87961502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc448d35",
   "metadata": {},
   "source": [
    "### Getting a Pretrained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e0b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fc2b2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 16:17:31.982936: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-28 16:17:31.995716: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759056452.011241 1034142 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759056452.015813 1034142 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1759056452.028074 1034142 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759056452.028090 1034142 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759056452.028091 1034142 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759056452.028092 1034142 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-28 16:17:32.032818: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"google-bert/bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(pretrained_model_name_or_path=\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbb41b9",
   "metadata": {},
   "source": [
    "### Understanding the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd997538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7fa76ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.pooler.dense.in_features, bert_model.pooler.dense.out_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b70191c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    [\"Example sentence 1\", \"Example sentence 2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "outputs = bert_model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "dict(outputs)['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22759261",
   "metadata": {},
   "source": [
    "The shape `torch.Size([2, 5, 768])` corresponds to the `last_hidden_state` output from BERT. Hereâ€™s what each dimension means:\n",
    "\n",
    "- `2`: The batch size (number of input sentences in the batch).\n",
    "- `5`: The sequence length (number of tokens in each input sentence, including special tokens like `[CLS]` and `[SEP]`).\n",
    "- `768`: The hidden size (the dimensionality of the BERT embeddings for each token; for `bert-base-uncased`, this is always 768).\n",
    "\n",
    "So, for each of the 2 input sentences, BERT outputs a sequence of 5 token embeddings, each of size 768."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cbe61e",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30763f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.x = [\n",
    "            tokenizer(\n",
    "                x,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=200,\n",
    "                padding_side='right',\n",
    "                return_tensors='pt',\n",
    "                return_token_type_ids=False\n",
    "            ) for x in X\n",
    "        ] \n",
    "        \n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccc02177",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(x_train, y_train)\n",
    "test_data = CustomDataset(x_test, y_test)\n",
    "# val_data = CustomDataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3a775ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict(train_data[2][0])['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3bc04b",
   "metadata": {},
   "source": [
    "### Building data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e3c37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_data_loader = DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2493c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[  101,  1045, 12524,  1045,  2572,  8025,  1011,  3756,  2013,  2026,\n",
       "           2678,  3573,  2138,  1997,  2035,  1996,  6704,  2008,  5129,  2009,\n",
       "           2043,  2009,  2001,  2034,  2207,  1999,  3476,  1012,  1045,  2036,\n",
       "           2657,  2008,  2012,  2034,  2009,  2001,  8243,  2011,  1057,  1012,\n",
       "           1055,  1012,  8205,  2065,  2009,  2412,  2699,  2000,  4607,  2023,\n",
       "           2406,  1010,  3568,  2108,  1037,  5470,  1997,  3152,  2641,  1000,\n",
       "           6801,  1000,  1045,  2428,  2018,  2000,  2156,  2023,  2005,  2870,\n",
       "           1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,\n",
       "           5436,  2003,  8857,  2105,  1037,  2402,  4467,  3689,  3076,  2315,\n",
       "          14229,  2040,  4122,  2000,  4553,  2673,  2016,  2064,  2055,  2166,\n",
       "           1012,  1999,  3327,  2016,  4122,  2000,  3579,  2014,  3086,  2015,\n",
       "           2000,  2437,  2070,  4066,  1997,  4516,  2006,  2054,  1996,  2779,\n",
       "          25430, 14728,  2245,  2055,  3056,  2576,  3314,  2107,  2004,  1996,\n",
       "           5148,  2162,  1998,  2679,  3314,  1999,  1996,  2142,  2163,  1012,\n",
       "           1999,  2090,  4851,  8801,  1998,  6623,  7939,  4697,  3619,  1997,\n",
       "           8947,  2055,  2037, 10740,  2006,  4331,  1010,  2016,  2038,  3348,\n",
       "           2007,  2014,  3689,  3836,  1010, 19846,  1010,  1998,  2496,  2273,\n",
       "           1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2054,\n",
       "           8563,  2033,  2055,  1045,  2572,  8025,  1011,  3756,  2003,  2008,\n",
       "           2871,  2086,  3283,  1010,  2023,  2001,  2641, 26932,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " tensor(0.))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7a4ce6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model(input_ids=torch.tensor(\n",
    "    [[  101,  1045, 12524,  1045,  2572,  8025,  1011,  3756,  2013,  2026,\n",
    "           2678,  3573,  2138,  1997,  2035,  1996,  6704,  2008,  5129,  2009,\n",
    "           2043,  2009,  2001,  2034,  2207,  1999,  3476,  1012,  1045,  2036,\n",
    "           2657,  2008,  2012,  2034,  2009,  2001,  8243,  2011,  1057,  1012,\n",
    "           1055,  1012,  8205,  2065,  2009,  2412,  2699,  2000,  4607,  2023,\n",
    "           2406,  1010,  3568,  2108,  1037,  5470,  1997,  3152,  2641,  1000,\n",
    "           6801,  1000,  1045,  2428,  2018,  2000,  2156,  2023,  2005,  2870,\n",
    "           1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,\n",
    "           5436,  2003,  8857,  2105,  1037,  2402,  4467,  3689,  3076,  2315,\n",
    "          14229,  2040,  4122,  2000,  4553,  2673,  2016,  2064,  2055,  2166,\n",
    "           1012,  1999,  3327,  2016,  4122,  2000,  3579,  2014,  3086,  2015,\n",
    "           2000,  2437,  2070,  4066,  1997,  4516,  2006,  2054,  1996,  2779,\n",
    "          25430, 14728,  2245,  2055,  3056,  2576,  3314,  2107,  2004,  1996,\n",
    "           5148,  2162,  1998,  2679,  3314,  1999,  1996,  2142,  2163,  1012,\n",
    "           1999,  2090,  4851,  8801,  1998,  6623,  7939,  4697,  3619,  1997,\n",
    "           8947,  2055,  2037, 10740,  2006,  4331,  1010,  2016,  2038,  3348,\n",
    "           2007,  2014,  3689,  3836,  1010, 19846,  1010,  1998,  2496,  2273,\n",
    "           1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2054,\n",
    "           8563,  2033,  2055,  1045,  2572,  8025,  1011,  3756,  2003,  2008,\n",
    "           2871,  2086,  3283,  1010,  2023,  2001,  2641, 26932,  1012,   102]]), \n",
    "        attention_mask=torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "          1, 1, 1, 1, 1, 1, 1, 1]]), return_dict=False)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06037e68",
   "metadata": {},
   "source": [
    "### Setting up HyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1ccc859",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f94dab8",
   "metadata": {},
   "source": [
    "### Building a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fb33460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.model_extrension = nn.Sequential(\n",
    "            nn.Linear(768, 384),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(384, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outs = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)[0][:, 0]\n",
    "        output = self.model_extrension(outs)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7793a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in list(bert_model.parameters()):\n",
    "    params.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98a57ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (model_extrension): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "    (1): Dropout(p=0.25, inplace=False)\n",
       "    (2): Linear(in_features=384, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SentimentModel(bert=bert_model).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37af81cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.6930, Accuracy: 0.5001\n",
      "Epoch 2/20, Loss: 0.6853, Accuracy: 0.5008\n",
      "Epoch 3/20, Loss: 0.6706, Accuracy: 0.5577\n",
      "Epoch 4/20, Loss: 0.6516, Accuracy: 0.6580\n",
      "Epoch 5/20, Loss: 0.6373, Accuracy: 0.7076\n",
      "Epoch 6/20, Loss: 0.6270, Accuracy: 0.7353\n",
      "Epoch 7/20, Loss: 0.6201, Accuracy: 0.7511\n",
      "Epoch 8/20, Loss: 0.6157, Accuracy: 0.7592\n",
      "Epoch 9/20, Loss: 0.6121, Accuracy: 0.7661\n",
      "Epoch 10/20, Loss: 0.6095, Accuracy: 0.7706\n",
      "Epoch 11/20, Loss: 0.6071, Accuracy: 0.7750\n",
      "Epoch 12/20, Loss: 0.6052, Accuracy: 0.7776\n",
      "Epoch 13/20, Loss: 0.6032, Accuracy: 0.7823\n",
      "Epoch 14/20, Loss: 0.6012, Accuracy: 0.7890\n",
      "Epoch 15/20, Loss: 0.6013, Accuracy: 0.7853\n",
      "Epoch 16/20, Loss: 0.6003, Accuracy: 0.7881\n",
      "Epoch 17/20, Loss: 0.5989, Accuracy: 0.7924\n",
      "Epoch 18/20, Loss: 0.5983, Accuracy: 0.7926\n",
      "Epoch 19/20, Loss: 0.5981, Accuracy: 0.7924\n",
      "Epoch 20/20, Loss: 0.5973, Accuracy: 0.7932\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "train_acc_plot = []\n",
    "train_loss_plot = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in train_data_loader:\n",
    "        input, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = input['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = input['attention_mask'].squeeze(1).to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = (outputs > 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = train_loss / len(train_data_loader)\n",
    "    accuracy = correct / total\n",
    "    train_loss_plot.append(avg_loss)\n",
    "    train_acc_plot.append(accuracy)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c25088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5913, Validation Accuracy: 0.8210\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "val_loss = 0\n",
    "val_correct = 0\n",
    "val_total = 0\n",
    "\n",
    "val_loss_plot = []\n",
    "val_acc_plot = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_data_loader:\n",
    "        inputs, labels = batch\n",
    "        input_ids = inputs['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(1).to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        preds = (outputs > 0.5).float()\n",
    "        val_correct += (preds == labels).sum().item()\n",
    "        val_total += labels.size(0)\n",
    "        \n",
    "        val_loss_plot.append(val_loss / val_total)\n",
    "        val_acc_plot.append(val_correct / BATCH_SIZE)\n",
    "\n",
    "avg_val_loss = val_loss / len(test_data_loader)\n",
    "val_accuracy = val_correct / val_total\n",
    "\n",
    "\n",
    "print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c22cb17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(texts):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            encoded = tokenizer(\n",
    "                text,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=200,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = encoded['input_ids'].to(device)\n",
    "            attention_mask = encoded['attention_mask'].to(device)\n",
    "            output = model(input_ids, attention_mask)\n",
    "            pred = (output > 0.5).float().item()\n",
    "            results.append(int(pred))\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "custom_texts = [\"This movie was awesome\", \"I did not like this film.\"]\n",
    "predictions = predict_sentiment(custom_texts)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08ddbb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"sentiment_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d597c204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"sentiment_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "predictions = predict_sentiment(custom_texts)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "23bq1a4274",
   "language": "python",
   "name": "23bq1a4274"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
